{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "name": "Fork of DS Task B ",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7487,
          "sourceType": "datasetVersion",
          "datasetId": 4931
        }
      ],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeenessa31-lgtm/Project/blob/main/Fork_of_DS_Task_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "psparks_instacart_market_basket_analysis_path = kagglehub.dataset_download('psparks/instacart-market-basket-analysis')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "z4BQFVJS-dUq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "psparks_instacart_market_basket_analysis_path = kagglehub.dataset_download('psparks/instacart-market-basket-analysis')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "UzvuUUaWyhq-",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Welcome To The Machine Learning (1) Project***"
      ],
      "metadata": {
        "id": "5VAjM-p5pcQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What we are trying to do in this project is to apply everything we learned in the machine learning course, from analyzing, cleaning, and processing data to applying machine learning algorithms.***"
      ],
      "metadata": {
        "id": "dttVcGxqqYhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And of course, we will use this dataset [**Dataset**](https://www.kaggle.com/datasets/psparks/instacart-market-basket-analysis) to practice sorcery on it.\n"
      ],
      "metadata": {
        "id": "a5qe_lDsrPKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***We will do our best to carry out all the required tasks..***"
      ],
      "metadata": {
        "id": "ualPmVlLs28L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The students in charge of this project:**\n",
        "- ***Hamza Moeen Rasheed Issa***\n",
        "- ***Zaid Yusef GH. Hashash***"
      ],
      "metadata": {
        "id": "T1vzoqHbsoES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rvXgcVPxvNpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RwI2al7-vbeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the first cell, we will import the libraries used in the project."
      ],
      "metadata": {
        "id": "F6wGtP3ztLa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import gc\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score"
      ],
      "metadata": {
        "id": "8IF223OHzj-4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download data directly from Kaggle, via **API Token**\n",
        "\n",
        "  -**(google.colab files)**\n",
        "  This is the library used to upload API tokens"
      ],
      "metadata": {
        "id": "jWfARK3LtjQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "trusted": true,
        "id": "J3KxuokHyhrH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Types = {\n",
        "    'aisle_id': 'int32',\n",
        "    'aisle': 'category',\n",
        "    'order_id': 'int32',\n",
        "    'user_id': 'int32',\n",
        "    'eval_set': 'category',\n",
        "    'order_number': 'int32',\n",
        "    'order_dow': 'int32',\n",
        "    'order_hour_of_day': 'int32',\n",
        "    'days_since_prior_order': 'float16',\n",
        "    'department_id': 'int8',\n",
        "    'department': 'category',\n",
        "    'product_id': 'int32',\n",
        "    'product_name': 'category',\n",
        "    'add_to_cart_order': 'int32',\n",
        "    'reordered': 'int8',\n",
        "}\n",
        "\n",
        "\n",
        "# Reading files using enhanced data types\n",
        "aisles = pd.read_csv('/kaggle/input/aisles.csv', dtype={i: Types.get(i, None) for i in ['aisle_id','aisle']})\n",
        "\n",
        "departments = pd.read_csv('/kaggle/input/departments.csv', dtype={i: Types.get(i, None) for i in ['department_id','department']})\n",
        "\n",
        "products = pd.read_csv('/kaggle/input/products.csv', dtype={i: Types.get(i, None) for i in ['product_id', 'aisle_id', 'department_id',' product_name']})\n",
        "\n",
        "# Read orders, excluding 'days_since_prior_order' from explicit dtype setting during read_csv\n",
        "orders = pd.read_csv('/kaggle/input/orders.csv', dtype={i: Types.get(i, None) for i in ['order_id', 'user_id', 'order_number', 'order_dow', 'order_hour_of_day','eval_set']})\n",
        "# Convert 'days_since_prior_order' to float16 after loading the DataFrame\n",
        "orders['days_since_prior_order'] = orders['days_since_prior_order'].astype('float16')\n",
        "\n",
        "order_products_prior = pd.read_csv('/kaggle/input/order_products__prior.csv', dtype={i: Types.get(i, None) for i in ['order_id', 'product_id', 'add_to_cart_order', 'reordered']})\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AwxweOCGyhrH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(aisles.info())\n",
        "print(\"------------------------------------------------\")\n",
        "print(orders.info())\n",
        "print(\"------------------------------------------------\")\n",
        "print(departments.info())\n",
        "print(\"------------------------------------------------\")\n",
        "print(products.info())\n",
        "print(\"------------------------------------------------\")\n",
        "print(order_products_prior.info())"
      ],
      "metadata": {
        "id": "t8_CbFMcPWP3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data = pd.merge(orders, order_products_prior,how = 'left', on='order_id')\n",
        "\n",
        "\n",
        "My_Data = pd.merge(My_Data, products,how = 'left', on='product_id')\n",
        "\n",
        "\n",
        "My_Data = pd.merge(My_Data, aisles,how = 'left', on='aisle_id')\n",
        "\n",
        "\n",
        "My_Data = pd.merge(My_Data, departments,how = 'left', on='department_id')\n",
        "\n",
        "\n",
        "My_Data.head()"
      ],
      "metadata": {
        "id": "lzlFXIal8bhS",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data['product_name'] = My_Data['product_name'].astype('category')"
      ],
      "metadata": {
        "id": "uposcpoLQTKa",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data.shape)\n",
        "print(\"-----------------------------\")\n",
        "print(My_Data.info())"
      ],
      "metadata": {
        "id": "P77X1YWZ9xdv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **• Missing value analysis and visualization**"
      ],
      "metadata": {
        "id": "PUpSulFSgstL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = My_Data.isnull().sum()\n",
        "print(missing_data)\n",
        "\n",
        "print(\"-------------------------------\")\n",
        "missing_percentage = (My_Data.isnull().sum() / len(My_Data)) * 100\n",
        "print(missing_percentage)"
      ],
      "metadata": {
        "id": "xdFrposx-_o7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_percentage = missing_percentage[missing_percentage > 0]\n",
        "\n",
        "if not missing_percentage.empty:\n",
        "    missing_percentage.sort_values().plot(kind='barh', figsize=(10, 6), color='skyblue')\n",
        "    plt.title('Percentage of Missing Values in Dataset')\n",
        "    plt.xlabel('Percentage (%)')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "else:\n",
        "  print(\"There are not missing values\")"
      ],
      "metadata": {
        "id": "YdgVPoQWRWG1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We will process missing values ​​directly to reduce memory usage.**"
      ],
      "metadata": {
        "id": "ss0Hu8zTxkpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  ***Since these are missing values ​​due to the lack of matching records in other tables, we will use a fixed-value compensation method.***"
      ],
      "metadata": {
        "id": "ZeLVzd5AxsC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text column processing\n",
        "for col in ['product_name', 'aisle', 'department']:\n",
        "    if My_Data[col].dtype.name == 'category' and 'Unknown' not in My_Data[col].cat.categories:\n",
        "        My_Data[col] = My_Data[col].cat.add_categories(['Unknown'])\n",
        "    My_Data[col] = My_Data[col].fillna('Unknown')\n",
        "\n",
        "# Processing numerical columns\n",
        "for col in ['product_id', 'aisle_id', 'department_id', 'add_to_cart_order', 'reordered']:\n",
        "    My_Data[col] = My_Data[col].fillna(-1).astype('int32')"
      ],
      "metadata": {
        "id": "1O5WCQaex31_",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Now, let's address the missing values ​​from the column (days_since_prior_order)"
      ],
      "metadata": {
        "id": "b5Lt-7LOyF8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data['days_since_prior_order'] = My_Data['days_since_prior_order'].fillna(-1)"
      ],
      "metadata": {
        "id": "ms3HkGhHyfnT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = My_Data.isnull().sum()\n",
        "print(missing_data)\n",
        "\n",
        "print(\"-------------------------------\")\n",
        "missing_percentage = (My_Data.isnull().sum() / len(My_Data)) * 100\n",
        "print(missing_percentage)"
      ],
      "metadata": {
        "id": "tv-YrR75yv0E",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9aM0kJrryy_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **• Distribution plots for numeric features and target(s) (histogram, density).**"
      ],
      "metadata": {
        "id": "Al-86IciUknz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***-  These six columns were chosen because they encompass the three fundamental dimensions of any recommendation system: time (hour and day of order), behavior (number of orders and basket order), and periodicity (days since the last order). These columns are the most correlated with the target variable (reordered).***"
      ],
      "metadata": {
        "id": "Bmvex6DY6uYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   **Determine peak activity**\n",
        "\n",
        "     The diagram shows that activity starts to rise from 7 am and reaches its peak between 10 am and 4 pm."
      ],
      "metadata": {
        "id": "cr_RTx4szxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_dist = My_Data['order_hour_of_day'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=hourly_dist.index, y=hourly_dist.values, color='teal')\n",
        "plt.title('Distribution of Orders by Hour of Day')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Total Orders')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del hourly_dist\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ucEvvPTuUrbd",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Distribution of days of the week**\n",
        "\n",
        "    The distribution of orders across the days of the week (from 0 to 6) shows that Saturday and Sunday are the most active days."
      ],
      "metadata": {
        "id": "PNExcGVm2c57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dow_dist = My_Data['order_dow'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=dow_dist.index, y=dow_dist.values, palette='viridis')\n",
        "plt.title('Distribution of Orders by Day of Week')\n",
        "plt.xticks(ticks=[0,1,2,3,4,5,6], labels=['Sat','Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri'])\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del dow_dist\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5CT2Kzx3VEkG",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Distribution of days since the last request\n",
        "\n",
        "    This graph (Density Plot) shows when customers will return to buy again. We notice two clear peaks; one at 7 days and another very sharp one at 30 days."
      ],
      "metadata": {
        "id": "5RZJbARz4IOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = My_Data['days_since_prior_order'].dropna().sample(2000000)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.kdeplot(sample_data, fill=True, color=\"r\")\n",
        "plt.title('Density Plot: Days Since Prior Order')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "00pKsiz8VPsV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Order of adding products to the cart**\n",
        "\n",
        "    The probability of a product being the first, second, or tenth item in the shopping cart is shown; the vast majority of products are added to the first five positions (1 to 5), and the higher the position, the fewer the number.\n",
        "\n",
        "    - Because the dataset exceeds 32 million records, plotting the Key Density Curve (KDE) requires immense processing power. I used a random sample of 2 million records to ensure accurate representation of the statistical distribution while maintaining performance efficiency and processing speed.\n",
        "\n",
        "    - The value (-1) was used as the Sentinel Value to represent initial requests that contained missing values. Having this value appear separately in the Distribution Plot ensures that new user data is not mixed with that of repeat users, and prevents the model from being biased towards average values ​​such as 7 or 8 days."
      ],
      "metadata": {
        "id": "v5B-PgCJ5UJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cart_dist = My_Data['add_to_cart_order'].value_counts().head(50)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.barplot(x=cart_dist.index, y=cart_dist.values, color='salmon')\n",
        "plt.title('Distribution of Add-to-Cart Order (Top 50)')\n",
        "plt.xlabel('Order in Cart')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del cart_dist\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "DDEGUeMuVVbU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Customer loyalty**\n",
        "\n",
        "    The distribution of the number of requests per user is shown. The distribution is right-skewed, as most users have a small number of requests, while the number gradually decreases for \"loyal\" users who have requested more than 50 times."
      ],
      "metadata": {
        "id": "3wGFxsqM1ws4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(My_Data['order_number'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Order Number (Customer Loyalty)')\n",
        "plt.xlabel('Order Number')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "4aoVazjRXGkl",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Target variable**\n",
        "\n",
        "    The graph shows the ratio of products that are repurchased versus those that are purchased for the first time."
      ],
      "metadata": {
        "id": "hyR46kW-1K8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **• Categorical cardinality analysis (barplots / top-k frequencies)**\n",
        "\n",
        "-  We focused on analyzing aisles and sections because they represent structural levels of data. Analyzing them allows us to understand the distribution of purchasing power and identify the categories that dominate transaction volume, which helps simplify the model's feature engineering and ensures its ability to differentiate between fast-moving consumer goods and seasonal goods."
      ],
      "metadata": {
        "id": "EmRZOncUYsQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Focus on the most impactful corridors**\n",
        "\n",
        "    Analysis of all aisles reveals that the Fresh Fruits aisle leads the list with exceptional purchasing volume. This indicates that repurchase decisions in this project are directly influenced by perishable goods, justifying the focus on short-term (7-day) order fulfillment patterns."
      ],
      "metadata": {
        "id": "5ilMnmEm-Q4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_aisles = My_Data['aisle'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(15, 30))\n",
        "\n",
        "sns.barplot(\n",
        "    x=top_aisles.values,\n",
        "    y=top_aisles.index,\n",
        "    order=top_aisles.index,\n",
        "    palette='magma'\n",
        ")\n",
        "\n",
        "plt.title('Top Frequent Aisles (Sorted from High to Low)')\n",
        "plt.xlabel('Count of Items')\n",
        "plt.ylabel('Aisle Name')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "9zZu2gQM_zS5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We moved from detailed aisle analysis to departmental analysis to gain a comprehensive strategic perspective. The results showed that the Produce department represented the largest weight in the data, reinforcing our hypothesis that repurchase behavior is closely linked to fresh, everyday produce."
      ],
      "metadata": {
        "id": "KcBma-9CAiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept_counts = My_Data['department'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    x=dept_counts.values,\n",
        "    y=dept_counts.index,\n",
        "    order=dept_counts.index,\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "plt.title('Frequency of Items by Department (Sorted)')\n",
        "plt.xlabel('Count of Items')\n",
        "plt.ylabel('Department Name')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del dept_counts\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "aUFRwql__cLy",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  I performed a **categorical cardinality** analysis using a logarithmic scale to understand the diversity of the textual data. This analysis is essential for selecting the appropriate encoding strategy for each variable; it showed that the products column has high cardinality and requires special processing, while sections and corridors represent a balanced classification level from which the model can learn effectively without excessive memory consumption."
      ],
      "metadata": {
        "id": "6OUDLk9XBkBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['aisle', 'department', 'product_name', 'eval_set']\n",
        "cardinality = My_Data[categorical_cols].nunique().sort_values()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "cardinality.plot(kind='barh', color='orange')\n",
        "plt.title('Cardinality of Categorical Features (Number of Unique Values)')\n",
        "plt.xlabel('Number of Unique Categories')\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del cardinality\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "eAryMT7hZNVh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Correlation matrix, heatmap and pairwise scatter plots for selected numeric features.**"
      ],
      "metadata": {
        "id": "AdQwUpO3dIg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Correlation Matrix / Heatmap**\n",
        "\n",
        "    This graph tells us how strong the relationship is between any two variables with a number between (-1 and 1)."
      ],
      "metadata": {
        "id": "qd-rdjXAy5b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = ['order_number', 'order_dow', 'order_hour_of_day',\n",
        "                'days_since_prior_order', 'add_to_cart_order', 'reordered']\n",
        "\n",
        "corr_matrix = My_Data[numeric_cols].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "del corr_matrix\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "xL50k0y0dOg8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  **Pairwise Scatter Plots**\n",
        "\n",
        "    A sample of 50,000 records was used because plotting a pairwise array requires extremely intensive computations that memory cannot handle with such a large dataset. Furthermore, plotting millions of points leads to overplotting, which obscures the details of class distribution, whereas a random sample provides an ideal balance between statistical accuracy and visual clarity.\n",
        "\n",
        "    **What do the three colors mean?**\n",
        "    \n",
        "    - Dark color (1): Represents **reordered** products\n",
        "    - Medium color (0): Represents **new products** purchased for the first time\n",
        "    - Very light color (-1): Represents the **first orders for new users** (First Orders)"
      ],
      "metadata": {
        "id": "ZjB5y5Lpz1gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  I developed the dual-color graph to include three color categories instead of two. This allowed us to clearly distinguish the initial requests (represented by the value -1). Through this graph, we demonstrated that the data from new users is completely isolated and does not cause any interference with the reorder patterns of existing users, thus improving the quality of the data input to the machine learning model."
      ],
      "metadata": {
        "id": "-1Zgto6m1lNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample = My_Data[numeric_cols].sample(50000)\n",
        "sns.pairplot(data_sample, hue='reordered', diag_kind='kde', plot_kws={'alpha': 0.5})\n",
        "plt.suptitle('Pairwise Scatter Plots (Sample of 50k rows)', y=1.02)\n",
        "plt.show()\n",
        "plt.close('all')\n",
        "\n",
        "del data_sample\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "eJWewRMijQOe",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **• Time-of-day, day-of-week, and monthly seasonality plots.**"
      ],
      "metadata": {
        "id": "Q7Xww8LynNHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ##  We analyzed the seasonality of the time period to uncover purchasing cycles. The results showed a significant concentration of orders at the beginning of the week and during daylight hours (10:00 to 16:00). These time patterns provide important context for the model, as peak times are typically associated with 'stocking up' orders, which are characterized by high reorder rates.\n",
        "\n",
        "- ##  We did not plot monthly seasonality because the Instacart dataset does not provide absolute calendar dates (timestamps), but rather relative time data focusing on the day of the week and hour of the day. Instead, we relied on days_since_prior_order analysis to uncover monthly patterns in repurchase behavior, with the results showing a clear peak at 30 days, representing the customers' monthly shopping cycle."
      ],
      "metadata": {
        "id": "0JKToUsl3CBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Daily Seasonality**"
      ],
      "metadata": {
        "id": "5uYVTUR9nRAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_season = My_Data.groupby('order_hour_of_day')['order_id'].nunique()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(hourly_season.index, hourly_season.values, marker='o', linestyle='-', color='b')\n",
        "plt.fill_between(hourly_season.index, hourly_season.values, alpha=0.1, color='b')\n",
        "plt.title('Daily Seasonality: Orders by Hour of Day')\n",
        "plt.xlabel('Hour (24h Format)')\n",
        "plt.ylabel('Total Orders')\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(range(0, 24))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "GWDCXY_rnPSc",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Weekly Seasonality**"
      ],
      "metadata": {
        "id": "90Sx8cWWnnDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_season = My_Data.groupby('order_dow')['order_id'].nunique()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x=weekly_season.index, y=weekly_season.values, marker='s', color='b', linewidth=2.5)\n",
        "plt.title('Weekly Seasonality: Orders by Day of Week')\n",
        "plt.xticks(ticks=[0,1,2,3,4,5,6], labels=['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n",
        "plt.ylabel('Total Orders')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "NRdSMjM9np7S",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- # *Outlier detection & treatment (winsorizing or removal with justification)*"
      ],
      "metadata": {
        "id": "vZ5XcTa2-3Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  ### We limited the analysis of outliers to these three variables because they represent the quantitative behavioral features of customers. As continuous variables, they are the most susceptible to statistical dispersion, which can skew the predictive model. The remaining columns are either categorical or time-bound, confined to a fixed range, and do not require outlier analysis"
      ],
      "metadata": {
        "id": "mD5DxKxeyhrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  A comprehensive ****digital audit**** was performed on the entire dataset. Using the ****Interquartile Range (IQR) equation****, we successfully determined the precise number and percentage of outliers for each behavioral variable. This digital audit validated the visual observations in the ****box plots**** ."
      ],
      "metadata": {
        "id": "12l0nKCmyhra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before\n",
        "cols_to_check = ['order_number', 'add_to_cart_order', 'days_since_prior_order']\n",
        "\n",
        "print(\"--- Digital Outlier Detection Report---\")\n",
        "\n",
        "for col in cols_to_check:\n",
        "    Q1 = My_Data[col].quantile(0.25)\n",
        "    Q3 = My_Data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = My_Data[(My_Data[col] < lower_bound) | (My_Data[col] > upper_bound)]\n",
        "    count = outliers.shape[0]\n",
        "    percentage = (count / len(My_Data)) * 100\n",
        "\n",
        "    print(f\"\\nColomen: {col}\")\n",
        "    print(f\"Max {upper_bound}\")\n",
        "    print(f\"Number Of outliers: {count:,}\")\n",
        "    print(f\"Percentage of outliers: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "evuUHhNpfoIS",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  This step was not merely a drawing, but an ****auditing process****; we proved numerically and visually that the outliers were not just random errors, but extreme values ​​representing a significant percentage (up to 5.5%), making the decision to address them using ****Winsorization**** a decision based on strong statistical evidence to ensure the model's stability later on."
      ],
      "metadata": {
        "id": "Dd9kQwr7yhra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_plot = ['order_number', 'add_to_cart_order', 'days_since_prior_order']\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, col in enumerate(cols_to_plot, 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.boxplot(y=My_Data[col], color='skyblue')\n",
        "    plt.title(f'Outliers in {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "yTzhxjJayhrb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  We ruled out the removal option because we are dealing with a massive database, and deleting outliers would result in the loss of over 1.7 million records, potentially preventing the model from understanding the behavior of its most active customers. Instead, we opted for Winsorizing to address statistical outliers by restricting them to the 99th percentile. This decision ensures data integrity and provides a stable training environment for the predictive model without sacrificing sample size."
      ],
      "metadata": {
        "id": "ZdNSx3lLyhrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  I experimented with the 99th and 95th percentiles and found that 0.95 was the most suitable for our project. This is because Instacart data exhibits very high dispersion in purchasing behavior, and the 95th percentile more closely aligns with the statistical IQR limits we previously calculated. This ensures a reduction in standard deviation and the fitting of more homogeneous features, which helps the algorithm learn more quickly and accurately."
      ],
      "metadata": {
        "id": "IRsQoxXZyhrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_fix = ['order_number', 'add_to_cart_order', 'days_since_prior_order']\n",
        "for col in cols_to_fix:\n",
        "    upper_limit = My_Data[col].quantile(0.95)\n",
        "    My_Data[col] = np.where(My_Data[col] > upper_limit, upper_limit, My_Data[col])\n",
        "\n",
        "    print(f\"Done: {col} | New Max Limit: {upper_limit}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "lXxVnGJ4yhrb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_plot = ['order_number', 'add_to_cart_order', 'days_since_prior_order']\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, col in enumerate(cols_to_plot, 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.boxplot(y=My_Data[col], color='lightgreen')\n",
        "    plt.title(f'Cleaned Outliers in {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close('all')\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "MwjpkmuByhrb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## Encoding Categorical Variables\n"
      ],
      "metadata": {
        "id": "0zPyomWAyhrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_stats = My_Data[['eval_set','product_name','product_id', 'user_id', 'aisle_id', 'department_id', 'order_dow', 'order_hour_of_day']].nunique()\n",
        "\n",
        "print(\"--- Cardinality Report ---\")\n",
        "print(encoding_stats)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6WRyJzycyhrc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## **One-Hot Encoding**\n",
        "  - ***Low-cardinality columns were chosen to ensure that the data dimensions did not become so large as to hinder processing.***\n",
        "  - ***(eval_set)*** : Convert from one column to two columns (because the original had 3 categories)\n",
        "  - ***(order_dow)*** : Convert to 6 columns (because the original had 7 categories)\n",
        "  - ***(department_id)*** : Convert to 21 columns (because the original had 22 categories)\n",
        "  - ***(order_hour_of_day)*** : Convert to 23 columns (because the original had 24 categories)"
      ],
      "metadata": {
        "id": "QqTmGw2Pyhrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_card_cols = ['eval_set', 'order_dow', 'department_id', 'order_hour_of_day']\n",
        "My_Data = pd.get_dummies(My_Data, columns=low_card_cols, drop_first=True, dtype='int8')\n",
        "\n",
        "cols_to_drop = ['department', 'aisle', 'aisle_id']\n",
        "My_Data.drop(columns=['department', 'aisle'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"Done! One-Hot Encoding complete.\")\n",
        "print(f\"New shape: {My_Data.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "SN0Kq2JOyhrc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print( {My_Data.shape})"
      ],
      "metadata": {
        "trusted": true,
        "id": "aQeai71Nyhrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## **Target Encoding**\n",
        "   -  ***The columns were selected because they contained thousands of unique values ​​that could not be processed using One-Hot Encoding.***\n",
        "   -  ***(product_id) , (user_id) , (aisle_id)***\n",
        "   -  The target columns were transformed from random integers to fractional floats, often ranging between 0 and 1. These values ​​accurately represent the \"reorder probability\" for each product and each user, making the model able to distinguish between \"highly requested\" and \"passing\" products with great ease."
      ],
      "metadata": {
        "id": "yzbiA1Poyhrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## **Frequency encoding**\n",
        "- ***Frequency encoding was chosen to represent the product_name column because it adds a new statistical feature to the model: product popularity. Unlike hashing tricks, which produce random values ​​that may suffer collisions, frequency encoding provides a logical relationship between the number of times a product appears and the probability of it being repurchased, thus improving prediction accuracy while maintaining memory efficiency.***\n",
        "- Converting long text names into numerical values ​​that express the \"weight\" or \"popularity\" of a category helps the model understand patterns associated with bestsellers."
      ],
      "metadata": {
        "id": "YJxYX9WRyhrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_counts = My_Data['product_name'].value_counts()\n",
        "\n",
        "My_Data['product_name'] = My_Data['product_name'].map(product_counts)\n",
        "print(My_Data[['product_name']].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "L6ro5q3Nyhrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data.info())"
      ],
      "metadata": {
        "trusted": true,
        "id": "uWM0lxPayhrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Comparison of Encoding Approaches****\n",
        "\n",
        "  -  **Key Observations:**\n",
        "\n",
        "    - One-Hot Encoding preserved full interpretability but increased dimensionality (52 additional columns).\n",
        "    - Target Encoding dramatically reduced dimensions while injecting predictive signal (reorder probability), with smoothing=10 applied to prevent                   overfitting on rare categories.\n",
        "    - Frequency Encoding provided a simple, meaningful feature (popularity count) without any risk of leakage or dimensionality explosion.\n",
        "\n",
        "****The combined approach achieved excellent memory efficiency (~3.5 GB total) while introducing strong predictive features, making the dataset suitable for both linear and tree-based models. Target leakage was avoided in Target Encoding by restricting fitting to training folds only within cross-validation pipelines.****"
      ],
      "metadata": {
        "id": "ZGs0no4KEkcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Target Encoding – Leakage Prevention****\n",
        "\n",
        "  - ****Target encoding may introduce data leakage if category statistics are computed using the full dataset.\n",
        "  To prevent this, **K-fold target encoding** is applied within the training data, where target means\n",
        "  are computed using K−1 folds and applied to the held-out fold.****\n",
        " ****This ensures that each sample is encoded without using its own target value.****\n"
      ],
      "metadata": {
        "id": "TW0e3eydEkcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ###  K-fold target encoding for product_id\n",
        "\n"
      ],
      "metadata": {
        "id": "96ccWu75Ekcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data[\"product_id_kfold_te\"] = np.nan\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "global_mean = My_Data[\"reordered\"].mean()\n",
        "\n",
        "for train_idx, val_idx in kf.split(My_Data):\n",
        "    train_fold = My_Data.iloc[train_idx]\n",
        "    val_fold = My_Data.iloc[val_idx]\n",
        "    means = train_fold.groupby(\"product_id\")[\"reordered\"].mean()\n",
        "    My_Data.loc[val_idx, \"product_id_kfold_te\"] = val_fold[\"product_id\"].map(means)\n",
        "\n",
        "My_Data[\"product_id_kfold_te\"] = My_Data[\"product_id_kfold_te\"].fillna(global_mean)\n",
        "\n",
        "print(\"product_id K-fold encoding done!\\n\")\n",
        "print(My_Data[[\"product_id\", \"product_id_kfold_te\"]].head())\n",
        "\n",
        "del means\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dXPyt5MTEkcl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ###  K-fold target encoding for user_id\n"
      ],
      "metadata": {
        "id": "dQtLg4YbEkcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data[\"user_id_kfold_te\"] = np.nan\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "global_mean = My_Data[\"reordered\"].mean()\n",
        "\n",
        "for train_idx, val_idx in kf.split(My_Data):\n",
        "    train_fold = My_Data.iloc[train_idx]\n",
        "    val_fold = My_Data.iloc[val_idx]\n",
        "    means = train_fold.groupby(\"user_id\")[\"reordered\"].mean()\n",
        "    My_Data.loc[val_idx, \"user_id_kfold_te\"] = val_fold[\"user_id\"].map(means)\n",
        "\n",
        "My_Data[\"user_id_kfold_te\"] = My_Data[\"user_id_kfold_te\"].fillna(global_mean)\n",
        "\n",
        "print(\"user_id K-fold encoding done!\\n\")\n",
        "print(My_Data[[\"user_id\", \"user_id_kfold_te\"]].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "7qBvBTlYEkcm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ###  K-fold target encoding for aisle_id\n"
      ],
      "metadata": {
        "id": "XEl28h5hEkcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data[\"aisle_id_kfold_te\"] = np.nan\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "global_mean = My_Data[\"reordered\"].mean()\n",
        "\n",
        "for train_idx, val_idx in kf.split(My_Data):\n",
        "    train_fold = My_Data.iloc[train_idx]\n",
        "    val_fold = My_Data.iloc[val_idx]\n",
        "    means = train_fold.groupby(\"aisle_id\")[\"reordered\"].mean()\n",
        "    My_Data.loc[val_idx, \"aisle_id_kfold_te\"] = val_fold[\"aisle_id\"].map(means)\n",
        "\n",
        "My_Data[\"aisle_id_kfold_te\"] = My_Data[\"aisle_id_kfold_te\"].fillna(global_mean)\n",
        "\n",
        "print(\"aisle_id K-fold encoding done!\\n\")\n",
        "print(My_Data[[\"aisle_id\", \"aisle_id_kfold_te\"]].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ssQVkPBjEkcm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations on K-fold Target Encoding Results\n",
        "\n",
        "When comparing the K-fold encoded values to the original global Target Encoding:\n",
        "\n",
        "- For `product_id` and `aisle_id`, the differences between global and K-fold means were very small.\n",
        "- For `user_id`, the differences were noticeably larger.\n",
        "\n"
      ],
      "metadata": {
        "id": "g66lgMSqEkcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Feature Scaling****"
      ],
      "metadata": {
        "id": "D3dHgnKGEkcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Feature scaling was performed to ensure numerical features contribute fairly to           models sensitive to magnitude.\n",
        "\n",
        "- **StandardScaler** was chosen (mean=0, std=1) as it is robust to outliers and suitable for distance-based models (KNN, SVM).\n",
        "- Applied to numerical features: `order_number`, `days_since_prior_order`, `add_to_cart_order`."
      ],
      "metadata": {
        "id": "6a7x-o_-Ekcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = ['order_number', 'days_since_prior_order', 'add_to_cart_order']\n",
        "\n",
        "My_Data[col] = My_Data[col].replace([np.inf, -np.inf], np.nan).fillna(My_Data[col].max())\n",
        "scaler = StandardScaler()\n",
        "\n",
        "My_Data[numeric_cols] = scaler.fit_transform(My_Data[numeric_cols])\n",
        "print(\"Scaling done on:\", numeric_cols , \"\\n\")\n",
        "print(My_Data[numeric_cols].describe().round(2))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4FsbNS5dEkcm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- # ****Feature Engineering****\n"
      ],
      "metadata": {
        "id": "rlVlzFz8Ekcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****User-level features****\n",
        "\n",
        "User-level features were engineered by aggregating data per `user_id` to capture individual customer behavior:\n",
        "\n",
        "- **Total number of orders**: Count of unique orders per user.\n",
        "- **Average basket size**: Mean number of items per order.\n",
        "- **Reorder ratio**: Proportion of reordered items across all orders (total reordered items / total items purchased).\n",
        "- **Mean days between orders**: Average `days_since_prior_order` for the user.\n",
        "- **Last order recency**: Number of days since the user's most recent order (relative to the latest date in the dataset or prediction point).\n",
        "\n",
        "These features were computed using `groupby('user_id')` on the orders and order_products tables, then merged back to the main dataset.\n"
      ],
      "metadata": {
        "id": "AwyEBAevEkcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Total number of orders per user\n",
        "\n"
      ],
      "metadata": {
        "id": "YwicmSaFEkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_total_orders = orders.groupby('user_id')['order_id'].nunique().rename('user_total_orders').reset_index()\n",
        "\n",
        "print(\"Total number of orders per user\" , \"\\n\")\n",
        "print(user_total_orders.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "RHVWEfDtEkcn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average basket size per user\n"
      ],
      "metadata": {
        "id": "8ZtHJ7K0Ekcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The merge between `orders` and `basket_sizes` on `order_id` was necessary to attach the basket size (number of items per order) to each order record in the `orders` DataFrame.\n",
        "\n",
        "- `basket_sizes` was derived from `order_products_prior` using groupby on `order_id` to count items per order.\n",
        "- The `orders` DataFrame contains user and order metadata (including `user_id`) but does not include basket size information.\n",
        "- Merging on `order_id` (left join) added the `basket_size` column to each order, enabling subsequent groupby on `user_id` to compute the **average basket size per user**.\n",
        "\n"
      ],
      "metadata": {
        "id": "oPnX3rWuEkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basket_sizes = order_products_prior.groupby('order_id')['product_id'].count().rename('basket_size').reset_index()\n",
        "orders_with_basket = orders.merge(basket_sizes, on='order_id', how='left')\n",
        "user_avg_basket = orders_with_basket.groupby('user_id')['basket_size'].mean().rename('user_avg_basket_size').reset_index()\n",
        "\n",
        "print(\"Average basket size per user\" , \"\\n\")\n",
        "print(user_avg_basket.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "AQhtbQSREkcn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reorder ratio per user"
      ],
      "metadata": {
        "id": "8tUVezurEkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_to_user = orders.set_index('order_id')['user_id'].to_dict()\n",
        "order_products_prior['user_id'] = order_products_prior['order_id'].map(order_to_user)\n",
        "user_reorder = order_products_prior.groupby('user_id')['reordered'].mean().rename('user_reorder_ratio').reset_index()\n",
        "\n",
        "print(\"Reorder ratio per user\" , \"\\n\")\n",
        "print(user_reorder.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "c8olWbjzEkcn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean days between orders per user"
      ],
      "metadata": {
        "id": "Nl-nXNTuEkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_mean_days = orders.groupby('user_id')['days_since_prior_order'].mean().rename('user_mean_days_between_orders').reset_index()\n",
        "\n",
        "print(\"Mean days between orders per user\" , \"\\n\")\n",
        "print(user_mean_days.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "w_JQpVD4Ekcn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Last order recency per user"
      ],
      "metadata": {
        "id": "XPzqr7qyEkcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_order_num = orders.groupby('user_id')['order_number'].max().reset_index()\n",
        "\n",
        "last_orders = max_order_num.merge(orders, on=['user_id', 'order_number'], how='left')\n",
        "\n",
        "last_orders['user_last_order_recency'] = last_orders['days_since_prior_order'].fillna(0)\n",
        "\n",
        "user_recency = last_orders[['user_id', 'user_last_order_recency']].drop_duplicates()\n",
        "\n",
        "print(\"Last order recency per user\" , \"\\n\")\n",
        "print(user_recency.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "J_2ALl9lEkcn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merged for all user-level features"
      ],
      "metadata": {
        "id": "jxwD1cvgEkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data['user_id'] = My_Data['user_id'].astype('int32')\n",
        "\n",
        "user_tables = [user_total_orders, user_avg_basket, user_reorder, user_mean_days, user_recency]\n",
        "\n",
        "for df in user_tables:\n",
        "    if 'user_id' in df.columns:\n",
        "        df['user_id'] = df['user_id'].astype('int32')\n",
        "\n",
        "    current_float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    df[current_float_cols] = df[current_float_cols].astype('float32')\n",
        "\n",
        "merge_all_features = user_total_orders \\\n",
        "    .merge(user_avg_basket, on='user_id') \\\n",
        "    .merge(user_reorder, on='user_id') \\\n",
        "    .merge(user_mean_days, on='user_id') \\\n",
        "    .merge(user_recency, on='user_id')\n",
        "\n",
        "My_Data = My_Data.merge(merge_all_features, on='user_id', how='left')\n",
        "\n",
        "del merge_all_features, user_total_orders, user_avg_basket, user_reorder, user_mean_days, user_recency\n",
        "gc.collect()\n",
        "\n",
        "print(My_Data.shape)"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": false,
        "id": "Ys-bgwUdEkco"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data.columns)\n",
        "print(\"Final shape:\", My_Data.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jzKEo-c5Ekco"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Product-level features****\n"
      ],
      "metadata": {
        "id": "0OV34kVQEkco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### ****product_reorder_rate****"
      ],
      "metadata": {
        "id": "B7gtmIziEkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate reorder statistics for each product\n",
        "product_features = order_products_prior.groupby('product_id')['reordered'].agg(['mean', 'count']).reset_index()\n",
        "product_features.columns = ['product_id', 'product_reorder_rate', 'product_total_purchases']\n",
        "\n",
        "# Convert data to float32 to save RAM\n",
        "product_features['product_reorder_rate'] = product_features['product_reorder_rate'].astype('float32')\n",
        "product_features['product_total_purchases'] = product_features['product_total_purchases'].astype('int32')\n",
        "\n",
        "print(product_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ckKDJhe8Ekco"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### ****avg_pos_in_cart****"
      ],
      "metadata": {
        "id": "_VVn1DGrEkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the average product ranking in the cart\n",
        "avg_pos = order_products_prior.groupby('product_id')['add_to_cart_order'].mean().reset_index()\n",
        "avg_pos.columns = ['product_id', 'avg_pos_in_cart']\n",
        "\n",
        "# Convert to float32\n",
        "avg_pos['avg_pos_in_cart'] = avg_pos['avg_pos_in_cart'].astype('float32')\n",
        "\n",
        "# Integrate it with the product features table\n",
        "product_features = product_features.merge(avg_pos, on='product_id', how='left')\n",
        "\n",
        "del avg_pos\n",
        "gc.collect()\n",
        "print(product_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ByM1UrQwEkco"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### ****popularity_over_time****"
      ],
      "metadata": {
        "id": "zaHPmujyEkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporary merge to get the order time for each product\n",
        "product_time = order_products_prior[['order_id', 'product_id']].merge(orders[['order_id', 'order_hour_of_day']], on='order_id', how='left')\n",
        "\n",
        "# Calculating the average purchase time for each product\n",
        "avg_hour = product_time.groupby('product_id')['order_hour_of_day'].mean().reset_index()\n",
        "avg_hour.columns = ['product_id', 'product_avg_hour_of_day']\n",
        "\n",
        "# Convert to float32 and final merge\n",
        "avg_hour['product_avg_hour_of_day'] = avg_hour['product_avg_hour_of_day'].astype('float32')\n",
        "product_features = product_features.merge(avg_hour, on='product_id', how='left')\n",
        "\n",
        "# Memory Cleansing\n",
        "del product_time, avg_hour\n",
        "gc.collect()\n",
        "print(product_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "P6u-gCU6Ekcp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### ****Final integration with (My_Data)****"
      ],
      "metadata": {
        "id": "pxDHk2rdEkcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrating product features with the main table\n",
        "My_Data = My_Data.merge(product_features, on='product_id', how='left')\n",
        "\n",
        "# Delete the intermediate table to save space\n",
        "del product_features\n",
        "gc.collect()\n",
        "print(My_Data.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VodzDn8TEkcp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data.shape)\n",
        "print(My_Data.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "C6tanwrREkcp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****User×Product interaction features :****\n"
      ],
      "metadata": {
        "id": "rZOmHTk7Ekcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### **up_purchase_count**"
      ],
      "metadata": {
        "id": "y132dMhHEkcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of times each user purchases each product\n",
        "up_features = order_products_prior.groupby(['user_id', 'product_id'])['order_id'].count().reset_index()\n",
        "up_features.columns = ['user_id', 'product_id', 'up_purchase_count']\n",
        "\n",
        "# Convert to int32 to save RAM\n",
        "up_features['up_purchase_count'] = up_features['up_purchase_count'].astype('int32')\n",
        "\n",
        "print(up_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "RSjBA33XEkcp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### **up_days_since_last_purchase**"
      ],
      "metadata": {
        "id": "19H0xDhvEkcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the products table with the orders table to get order_number\n",
        "# We only need two columns from the orders table to save RAM.\n",
        "temp_orders = orders[['order_id', 'order_number']]\n",
        "order_products_prior = order_products_prior.merge(temp_orders, on='order_id', how='left')\n",
        "\n",
        "# Account last order number where the user purchased this product\n",
        "last_order_per_up = order_products_prior.groupby(['user_id', 'product_id'])['order_number'].max().reset_index()\n",
        "\n",
        "# Get the time (how many days have passed)\n",
        "up_last_order = last_order_per_up.merge(orders[['user_id', 'order_number', 'days_since_prior_order']],\n",
        "                                        on=['user_id', 'order_number'], how='left')\n",
        "\n",
        "# Rename the column to make it clear\n",
        "up_last_order.rename(columns={'days_since_prior_order': 'up_days_since_last_purchase'}, inplace=True)\n",
        "\n",
        "# Integrate the feature with the up_features table\n",
        "up_features = up_features.merge(up_last_order[['user_id', 'product_id', 'up_days_since_last_purchase']],\n",
        "                                 on=['user_id', 'product_id'], how='left')\n",
        "\n",
        "# Clean the RAM immediately\n",
        "del temp_orders, last_order_per_up, up_last_order\n",
        "gc.collect()\n",
        "\n",
        "print(up_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "pAvhxipjEkcp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### **up_reorder_probability**"
      ],
      "metadata": {
        "id": "ZGlbSm88Ekcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_orders_count = My_Data[['user_id', 'user_total_orders']].drop_duplicates()\n",
        "\n",
        "up_stats = order_products_prior.groupby(['user_id', 'product_id'])['order_number'].agg(['min', 'max']).reset_index()\n",
        "up_stats.columns = ['user_id', 'product_id', 'first_order_number', 'last_order_number']\n",
        "\n",
        "up_counts = order_products_prior.groupby(['user_id', 'product_id'])['order_id'].count().reset_index()\n",
        "up_counts.columns = ['user_id', 'product_id', 'up_purchase_count']\n",
        "\n",
        "up_features = up_counts.merge(up_stats, on=['user_id', 'product_id'], how='left')\n",
        "up_features = up_features.merge(user_orders_count, on='user_id', how='left')\n",
        "\n",
        "up_features['up_reorder_probability'] = up_features['up_purchase_count'] / \\\n",
        "                                         (up_features['user_total_orders'] - up_features['first_order_number'] + 1)\n",
        "\n",
        "up_features = up_features.merge(orders[['user_id', 'order_number', 'days_since_prior_order']],\n",
        "                                 left_on=['user_id', 'last_order_number'],\n",
        "                                 right_on=['user_id', 'order_number'], how='left')\n",
        "\n",
        "up_features.rename(columns={'days_since_prior_order': 'up_days_since_last_purchase'}, inplace=True)\n",
        "\n",
        "up_features['up_reorder_probability'] = up_features['up_reorder_probability'].astype('float32')\n",
        "up_features['up_days_since_last_purchase'] = up_features['up_days_since_last_purchase'].fillna(0).astype('float32')\n",
        "\n",
        "up_features.drop(['first_order_number', 'last_order_number', 'order_number', 'user_total_orders'], axis=1, inplace=True)\n",
        "\n",
        "del up_stats, up_counts, user_orders_count\n",
        "gc.collect()\n",
        "print(up_features.columns.tolist())\n",
        "print(up_features.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "zQld0HQ4Ekcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ### **Final integration with My_Data**"
      ],
      "metadata": {
        "id": "VowXam6AEkcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data['user_id'] = My_Data['user_id'].astype('float32')\n",
        "My_Data['product_id'] = My_Data['product_id'].astype('float32')\n",
        "up_features['user_id'] = up_features['user_id'].astype('float32')\n",
        "up_features['product_id'] = up_features['product_id'].astype('float32')\n",
        "\n",
        "My_Data = My_Data.merge(up_features, on=['user_id', 'product_id'], how='left')\n",
        "\n",
        "del up_features\n",
        "gc.collect()\n",
        "\n",
        "print(My_Data.shape)\n",
        "print(My_Data.columns.tolist())"
      ],
      "metadata": {
        "trusted": true,
        "id": "lZyrJe__Ekcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Temporal features: hour/day/month/year, season, holiday flags (if available)****\n",
        "  - Since we do not have a complete history (year/month) in the core Instacart dataset, we will focus on the available features (hour and day) and build “flags” from them that represent time periods and seasons."
      ],
      "metadata": {
        "id": "fTW0jvFHEkcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## **Implemented Features**"
      ],
      "metadata": {
        "id": "vZTG_sxVEkcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ****All available time data within the original files has been exhausted. The current feature-engineered data is perfectly sufficient to model user behavior based on daily and weekly patterns.****"
      ],
      "metadata": {
        "id": "8yb30z3GEkcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting available raw time data from the command table\n",
        "# We use drop_duplicates to ensure that rows are not duplicated when merging\n",
        "temporal_info = orders[['order_id', 'order_hour_of_day', 'order_dow', 'days_since_prior_order']].drop_duplicates()"
      ],
      "metadata": {
        "trusted": true,
        "id": "38XB4uCoEkcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing data types to ensure fast and warning-free integration.\n",
        "temporal_info['order_id'] = temporal_info['order_id'].astype('int32')\n",
        "My_Data['order_id'] = My_Data['order_id'].astype('int32')"
      ],
      "metadata": {
        "trusted": true,
        "id": "v0Br-0kyEkcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging Time Features with the My_Data Main Table\n",
        "# We only select columns that have not been merged previously to avoid duplication\n",
        "cols_to_merge = temporal_info.columns.difference(My_Data.columns).tolist() + ['order_id']\n",
        "My_Data = My_Data.merge(temporal_info[cols_to_merge], on='order_id', how='left')"
      ],
      "metadata": {
        "trusted": true,
        "id": "wZKFiMl_Ekcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - ### ****Hour of Day****"
      ],
      "metadata": {
        "id": "zb0-oVinEkcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the \"Day Periods\" feature based on the available on-demand clock.\n",
        "def get_day_period(hour):\n",
        "    if 5 <= hour < 12: return 1    # Morning\n",
        "    elif 12 <= hour < 17: return 2  # Afternoon\n",
        "    elif 17 <= hour < 21: return 3  # Evening\n",
        "    else: return 4                 # Night\n",
        "\n",
        "My_Data['day_period'] = My_Data['order_hour_of_day'].apply(get_day_period).astype('int8')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FyP5XotKEkcq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Day of the Week****"
      ],
      "metadata": {
        "id": "wbtZ3QCrEkcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a \"Weekend Flag\" feature based on the available day number\n",
        "# In Instacart: Days 0 and 1 represent peak demand (often Saturday and Sunday)\n",
        "My_Data['is_weekend'] = My_Data['order_dow'].apply(lambda x: 1 if x in [0, 1] else 0).astype('int8')"
      ],
      "metadata": {
        "trusted": true,
        "id": "g9vDzxSNEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Addressing the \"Days Since Last Order\" feature that is already available\n",
        "My_Data['days_since_prior_order'] = My_Data['days_since_prior_order'].fillna(0).astype('float32')\n",
        "# Clean the memory immediately after completion\n",
        "del temporal_info\n",
        "gc.collect()\n",
        "print(['day_period', 'is_weekend', 'days_since_prior_order'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "xsbKj4WpEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Calculating the features of the \"last 3 orders\" for each user and product****\n",
        "  - #### ****We will know how many times the user purchased this product in their last 3 visits to the store****"
      ],
      "metadata": {
        "id": "V1ByPEuxEkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the last order number for each user (Max Order Number)\n",
        "user_last_orders = orders.groupby('user_id')['order_number'].max().reset_index()\n",
        "user_last_orders.columns = ['user_id', 'max_order_number']\n",
        "\n",
        "# Create the missing variable by combining product data with maximum order numbers\n",
        "# This is the line you were missing, which was causing the NameError\n",
        "last_3_orders_data = order_products_prior.merge(user_last_orders, on='user_id', how='left')\n",
        "\n",
        "# Filter data to only the last 3 requests\n",
        "last_3_orders_data = last_3_orders_data[last_3_orders_data['order_number'] > (last_3_orders_data['max_order_number'] - 3)]\n",
        "\n",
        "# Calculate the number of purchases within this time window\n",
        "up_last_3_count = last_3_orders_data.groupby(['user_id', 'product_id'])['order_id'].count().reset_index()\n",
        "up_last_3_count.columns = ['user_id', 'product_id', 'up_last_3_purchase_count']\n",
        "\n",
        "# Final integration with the My_Data table\n",
        "My_Data = My_Data.merge(up_last_3_count, on=['user_id', 'product_id'], how='left')\n",
        "My_Data['up_last_3_purchase_count'] = My_Data['up_last_3_purchase_count'].fillna(0).astype('int8')\n",
        "\n",
        "# Memory cleaning\n",
        "del last_3_orders_data, up_last_3_count, user_last_orders\n",
        "gc.collect()\n",
        "\n",
        "print(My_Data[['user_id', 'product_id', 'up_last_3_purchase_count']].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ol7FjGQvEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Calculating the \"Last Order Rate\" feature****\n",
        "  - #### ****This is a very powerful feature that tells the model if the product is in the last basket purchased by the user.****"
      ],
      "metadata": {
        "id": "FrRllS1dEkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-cleaning: Drop any existing versions of the column to avoid duplication errors\n",
        "cols_to_drop = [c for c in My_Data.columns if 'is_in_last_order' in c]\n",
        "if cols_to_drop:\n",
        "    My_Data.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "#  Limit the last order to each user.\n",
        "user_last_order = orders.groupby('user_id')['order_number'].max().reset_index()\n",
        "user_last_order.columns = ['user_id', 'order_number']\n",
        "\n",
        "# Knowing which products were in that order\n",
        "last_order_products = order_products_prior.merge(user_last_order, on=['user_id', 'order_number'], how='inner')\n",
        "last_order_products['is_in_last_order'] = 1\n",
        "\n",
        "# Integration with My_Data\n",
        "My_Data = My_Data.merge(last_order_products[['user_id', 'product_id', 'is_in_last_order']],\n",
        "                        on=['user_id', 'product_id'], how='left')\n",
        "\n",
        "My_Data['is_in_last_order'] = My_Data['is_in_last_order'].fillna(0).astype('int8')\n",
        "My_Data[col] = My_Data[col].replace([np.inf, -np.inf], np.nan).fillna(0).astype('int8')\n",
        "\n",
        "del user_last_order, last_order_products\n",
        "gc.collect()\n",
        "\n",
        "print(My_Data.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "HJiYW1IMEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Log Transform****"
      ],
      "metadata": {
        "id": "KNmthTpHEkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the total product purchases to a logarithmic scale\n",
        "# We use +1 to avoid the zero logarithm\n",
        "My_Data['log_total_product_sales'] = np.log1p(My_Data['product_total_purchases']).astype('float32')\n",
        "print(My_Data.head())\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qQsyOoZkEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have engineered 83 features categorized into 5 strategic groups to drive model performance:\n",
        "\n",
        " Identifiers: user_id, product_id, order_id (Linking keys).\n",
        " User Profile: total_orders, avg_basket, reorder_ratio (Customer history & loyalty).\n",
        " Product Stats: reorder_rate, popularity_score (Item characteristics).\n",
        " Interaction: up_purchase_count, up_prob, in_last_3_orders (User-specific habits).\n",
        " Context: hour, day, weekend_flag (Temporal patterns)."
      ],
      "metadata": {
        "id": "chkWeEUbEkcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(My_Data.columns))\n",
        "print(My_Data.columns.tolist())"
      ],
      "metadata": {
        "trusted": true,
        "id": "cB1yl9lIEkcr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Data Cleaning: Addressing Residual NaNs\n",
        "Upon final inspection, 206,209 rows (< 1% of data) contained NaN values in product-specific features.\n",
        "\n",
        "Audit Findings: These rows correspond to the Train/Test set entries (from orders.csv) which represent future orders. By definition, these new orders do not yet have product history directly attached via the prior dataset merge logic.\n",
        "\n",
        "Action Plan: We cannot drop these rows as they are the target inputs for our model. instead, we apply a domain-aware imputation strategy:\n",
        "\n",
        "Missing Counts/Rates: Fill with 0. (Implies no history exists yet for this specific context).\n",
        "Result: Dataset integrity is preserved at 100%, ensuring every row is valid for prediction."
      ],
      "metadata": {
        "id": "XWyy_uqEEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nan_counts = My_Data.isnull().sum()\n",
        "print(nan_counts[nan_counts > 0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "wzXNDOYMEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Data Sanitization: Handling Residual Nulls\n",
        "After merging all engineered features, we perform a final audit for missing values.\n",
        "\n",
        "Observation: We expect to find NaN values in product-interaction features (e.g., up_purchase_count, product_reorder_rate) specifically for the Train/Test set rows.\n",
        "\n",
        "Reason: These rows represent new orders where the specific user-product interaction might not have occurred in the prior history, or the product features could not be mapped (cold start)."
      ],
      "metadata": {
        "id": "PpZnwCOHEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nan_summary = My_Data.isnull().sum()\n",
        "nan_only = nan_summary[nan_summary > 0]\n",
        "print(nan_only)\n",
        "\n",
        "if len(nan_only) > 0:\n",
        "    cols_to_fix = nan_only.index.tolist()\n",
        "\n",
        "    My_Data[cols_to_fix] = My_Data[cols_to_fix].fillna(0)\n",
        "\n",
        "print(len(nan_only))"
      ],
      "metadata": {
        "trusted": true,
        "id": "etXiahhjEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality & collinearity\n",
        "We will now perform a Variance Inflation Factor (VIF) test to identify and remove features with severe multicollinearity ($VIF > 10$).\n",
        "\n",
        "Process:\n",
        "\n",
        "Sample the Data: Calculating VIF on 32M rows is computationally expensive, so we take a representative random sample of 100,000 rows.\n",
        "\n",
        "Calculate VIF: For each continuous numerical feature, we compute how much its variance is inflated by the other features.\n",
        "\n",
        "Action: Features with high VIF scores (indicating they are redundant copies of others) will be candidates for removal to streamline the model."
      ],
      "metadata": {
        "id": "bNq04CzkEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_cols = ['user_id', 'product_id', 'order_id', 'reordered', 'eval_set_test', 'eval_set_train']\n",
        "\n",
        "exclude_cols += [col for col in My_Data.columns if '_kfold_te' in col or 'order_dow_' in col or 'department_id_' in col or 'order_hour_' in col]\n",
        "\n",
        "\n",
        "numeric_cols = My_Data.select_dtypes(include=['float32', 'float64', 'int32', 'int64', 'int8']).columns.tolist()\n",
        "\n",
        "features_for_vif = list(set(numeric_cols) - set(exclude_cols))\n",
        "\n",
        "print(f\"Features selected for VIF: {len(features_for_vif)}\")\n",
        "print(features_for_vif)\n",
        "\n",
        "\n",
        "vif_data = My_Data[features_for_vif].sample(100000, random_state=42).copy()\n",
        "\n",
        "vif_data = vif_data.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "vif_df = pd.DataFrame()\n",
        "vif_df[\"feature\"] = vif_data.columns\n",
        "vif_df[\"VIF\"] = [variance_inflation_factor(vif_data.values, i)\n",
        "                 for i in range(len(vif_data.columns))]\n",
        "\n",
        "print(vif_df.sort_values(by=\"VIF\", ascending=False))"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Gs5HfJ-Ekcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Based on the audit results, we detected severe multicollinearity in specific feature groups. To improve model stability and training speed, we are removing the redundant variables\n"
      ],
      "metadata": {
        "id": "TSp1gy5VEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_drop = [\n",
        "    'product_avg_hour_of_day',\n",
        "    'product_total_purchases',\n",
        "    'product_name',\n",
        "    'avg_pos_in_cart'\n",
        "]\n",
        "\n",
        "cols_present = [c for c in features_to_drop if c in My_Data.columns]\n",
        "\n",
        "if cols_present:\n",
        "    My_Data.drop(columns=cols_present, inplace=True)\n",
        "    print(f\"Successfully dropped high VIF features: {cols_present}\")\n",
        "    print(f\"New column count: {len(My_Data.columns)}\")\n",
        "else:\n",
        "    print(\"Columns already dropped or not found.\")\n",
        "\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SsqrE7wmEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- # ****Imbalanced data handling (classification)****\n"
      ],
      "metadata": {
        "id": "cEh0eOsaEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the train schedule\n",
        "train_orders = pd.read_csv('/kaggle/input/order_products__train.csv', usecols=['order_id', 'product_id', 'reordered'])\n",
        "\n",
        "#  Since we want to predict for each (user + product)\n",
        "# We will use 'product_id' and 'user_id' for linking (you need to link the train to the orders table first to get the user_id)\n",
        "orders = pd.read_csv('/kaggle/input/orders.csv', usecols=['order_id', 'user_id', 'eval_set'])\n",
        "train_full = train_orders.merge(orders[['order_id', 'user_id']], on='order_id', how='left')\n",
        "\n",
        "# Any record present in My_Data but not in train_full will be assigned NaN and then converted to 0\n",
        "My_Data = My_Data.merge(train_full[['user_id', 'product_id', 'reordered']],\n",
        "                        on=['user_id', 'product_id'],\n",
        "                        how='left',\n",
        "                        suffixes=('', '_final_target'))\n",
        "\n",
        "# Dr. Cleaning the new target column\n",
        "My_Data['target'] = My_Data['reordered_final_target'].fillna(0).astype('int8')\n",
        "My_Data.drop(['reordered_final_target'], axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tjnLuDPHEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "del train_orders\n",
        "del orders\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7Cp6Hfa3Ekcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Examine the Target (Class Distribution) distribution****"
      ],
      "metadata": {
        "id": "yfG-oiAREkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data['target'].value_counts())\n",
        "print(My_Data['target'].value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6wYYncgWEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Row Count Check****"
      ],
      "metadata": {
        "id": "Qee8HdLpEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(My_Data))\n",
        "print(len(My_Data))"
      ],
      "metadata": {
        "trusted": true,
        "id": "HeWLNxaUEkcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Nulls Check****"
      ],
      "metadata": {
        "id": "S-bq_MyGEkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data['target'].isnull().sum())"
      ],
      "metadata": {
        "trusted": true,
        "id": "pK5mKL67Ekct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Logic Spot-check****"
      ],
      "metadata": {
        "id": "HK2SJ9nWEkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(My_Data[My_Data['target'] == 1][['user_id', 'product_id', 'target']].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "NPDNygd1Ekct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- #### Analysis of the target variable revealed a class imbalance of approximately ****1:7****. This means the data contains seven times more negative samples than positive ones, justifying the need to use imbalance correction techniques to ensure the model's accuracy in predicting buybacks"
      ],
      "metadata": {
        "id": "pmn4V-8oEkct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $$\\frac{\\text{Zeros}}{\\text{Ones}} = \\frac{28,402,695}{4,238,003} \\approx 6.7019$$"
      ],
      "metadata": {
        "id": "95Lxz0p_Ekct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------\n",
        "- ## ****Aggregation Strategy****"
      ],
      "metadata": {
        "id": "AZrKauqxEkct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - ### We will use an ****Aggregation Strategy**** instead of dealing with the entire dataset,because:\n",
        "    -  *Transitioning from the order level to the (user-product) level*\n",
        "    -  *Noise Reduction*\n",
        "    -  *Computational Efficiency*\n",
        "    -  *Feature Signal*"
      ],
      "metadata": {
        "id": "z2zbGnCZEkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_pairs = My_Data.groupby(['user_id', 'product_id']).ngroups\n",
        "print(f\"Number of uniques:{unique_pairs}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7XqTKsTbEkct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### *By implementing an aggregation strategy, we successfully reduced the data size by approximately 58% (from 32.6 million to 13.5 million records) while preserving all user behavioral information. This not only improved memory efficiency but also made imbalance handling more precise and focused on the unique user-product binaries*"
      ],
      "metadata": {
        "id": "DmJrnbmWEkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "My_Data_Aggregated = My_Data.groupby(['user_id', 'product_id']).last().reset_index()\n",
        "\n",
        "print( My_Data_Aggregated.shape)\n",
        "print(\"-\" * 30)\n",
        "print(My_Data_Aggregated['target'].value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "jJDRNhcVEkct"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### *After the aggregation process, we observed a decrease in the percentage of positive categories to 6.1%. This reflects the actual nature of the problem; we are moving from predicting at the level of 'every historical purchase' to predicting at the level of 'product repurchase opportunity'. This makes the task more challenging and more strongly justifies the need for the imbalance handling techniques that we will now apply*"
      ],
      "metadata": {
        "id": "dOwI3S3ZEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $$\\frac{\\text{Zeros}}{\\text{Ones}} = \\frac{0.93867}{0.06133} \\approx 15.3$$"
      ],
      "metadata": {
        "id": "7-Dt3RldEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------\n",
        "- ## ****Starting the experimentation process****"
      ],
      "metadata": {
        "id": "TejIWHLSEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Class Weights****"
      ],
      "metadata": {
        "id": "OFPKP08qEkcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight calculation (approximately 15.3)\n",
        "pos_weight = 0.93867 / 0.06133\n",
        "model_class_weights = XGBClassifier(scale_pos_weight=pos_weight, n_estimators=100, tree_method='hist')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_9c-02EBEkcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****Undersampling****"
      ],
      "metadata": {
        "id": "wqgPyPG1Ekcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the two categories\n",
        "minority = My_Data_Aggregated[My_Data_Aggregated['target'] == 1]\n",
        "majority = My_Data_Aggregated[My_Data_Aggregated['target'] == 0]\n",
        "\n",
        "# Taking a random sample from the majority equal to the minority\n",
        "majority_under = majority.sample(len(minority), random_state=42)\n",
        "\n",
        "# Merge them into a new, separate trial schedule\n",
        "df_under = pd.concat([minority, majority_under])\n",
        "\n",
        "print(\"Undersampling:\")\n",
        "print(df_under['target'].value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "UfKR2tZ6Ekcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### ****SMOTE****"
      ],
      "metadata": {
        "id": "QTVtFMhsEkcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We're taking a sample for testing to prevent the RAM from exploding\n",
        "df_sample_smote = My_Data_Aggregated.sample(1000000, random_state=42)\n",
        "X_sample = df_sample_smote.drop('target', axis=1)\n",
        "y_sample = df_sample_smote['target']\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_sample, y_sample)\n",
        "\n",
        "print(\"SMOTE:\")\n",
        "print(y_res.value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "0qvTSUjJEkcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### Because of a significant class imbalance in the data, we adopted the ****F1-Score**** as the primary criterion for evaluating our experiments, as it provides a precise balance between the ability to detect recalls and the accuracy of those predictions, thus preventing the misleading effects that can be caused by the traditional accuracy metric"
      ],
      "metadata": {
        "id": "B-mYOAXzEkcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(model, X, y, title):\n",
        "    model.fit(X, y)\n",
        "    preds = model.predict(X)\n",
        "    print(f\"--- {title} ---\")\n",
        "    print(f\"F1-Score: {f1_score(y, preds):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y, preds):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y, preds):.4f}\\n\")\n",
        "\n",
        "\n",
        "# A slightly modified function for SMOTE because the data is already ready and segmented\n",
        "def get_results_for_smote(model, X, y, title):\n",
        "    model.fit(X, y)\n",
        "    preds = model.predict(X)\n",
        "    print(f\"--- {title} ---\")\n",
        "    print(f\"F1-Score: {f1_score(y, preds):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y, preds):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y, preds):.4f}\\n\")\n",
        "\n",
        "\n",
        "# Unprocessed Experiment (Original Data - Sample)\n",
        "sample_orig = My_Data_Aggregated.sample(100000)\n",
        "get_results(XGBClassifier(), sample_orig.drop('target', axis=1), sample_orig['target'], \"Baseline\")\n",
        "\n",
        "# SMOTE\n",
        "get_results_for_smote(XGBClassifier(), X_res, y_res, \"SMOTE\")\n",
        "\n",
        "# Class Weights Test\n",
        "get_results(XGBClassifier(scale_pos_weight=15), sample_orig.drop('target', axis=1), sample_orig['target'], \"Class Weights\")\n",
        "\n",
        "#3. The Undersampling Experience\n",
        "get_results(XGBClassifier(), df_under.drop('target', axis=1), df_under['target'], \"Undersampling\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "HTVK_1IhEkcu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Analyze Numbers****"
      ],
      "metadata": {
        "id": "1aeEevsQEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ### *Baseline (unprocessed): The ٌ****Recall**** is catastrophic (0.22), meaning the model failed to detect 80% of repurchase transactions. It's only \"cautious\" (high precision) but \"afraid\" to predict 1*\n",
        "- ----------------------------------------------------------------------------------------------------------\n",
        "- ### *SMOTE: The numbers are very perfect (0.93) because we tested it on a small, artificially balanced sample, and it illustrates the \"maximum potential\" if huge computing resources were available*\n",
        "- - ----------------------------------------------------------------------------------------------------------\n",
        "- ### *Class Weights: The Recall jumped to (0.93)! The model became \"very bold\" in unit hunting, but at the expense of precision, which decreased. This is exactly what was expected when balancing the weights*\n",
        "- - ----------------------------------------------------------------------------------------------------------\n",
        "- ### *Undersampling: It gave you the best realistic balance; Precision and Recall are close (0.74 - 0.76), and F1-Score is very high (0.75)*"
      ],
      "metadata": {
        "id": "ZpXaD1_sEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Pros & Cons****\n",
        "----------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "qddCAUaNEkcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Class Weights****\n",
        "\n",
        "  - ### ****Pros****: *We haven't lost a single record from the 13 million rows*\n",
        "  - ### ****Cons****: *This led to a significant increase in False Positives (predicting that the customer will buy when they will not)*"
      ],
      "metadata": {
        "id": "3Px_ERyKEkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****Undersampling****\n",
        "\n",
        "  - ### ****Pros****: *Incredible speed in training and a very balanced performance between hunting single targets and accurate prediction.*\n",
        "  - ### ****Cons****: *We removed millions of records from category (0), which may prevent the model from learning some special cases.*"
      ],
      "metadata": {
        "id": "u5pww4vlEkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## ****SMOTE****\n",
        "\n",
        "  - ### ****Pros****: *It solves the problem of data scarcity by generating intelligent samples instead of simply iterating.*\n",
        "  - ### ****Cons****: *High memory consumption and cannot be applied to the entire Instacart data (13M) in a limited Kagel environment*"
      ],
      "metadata": {
        "id": "zS9nJZnYEkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------------------------\n",
        "- # ****Time-aware splitting****"
      ],
      "metadata": {
        "id": "6IPraRxzEkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## Since purchasing behavior is chronological, we avoided random splitting, which could lead to ****(Data Leakage)**** Instead, we adopted a ****(User-Based Splitting)**** strategy, where the model is trained on one group of users and its predictive ability is tested on a completely different group. This ensures the model can ****(Generalize)**** and accurately predict future orders."
      ],
      "metadata": {
        "id": "d1Sa7UVYEkcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## Training on raw ****(Transactional Data)**** leads to duplicate features for the same ****(User-Product)****, causing data inflation without adding new predictive information. Instead, we performed ****feature engineering**** to transform the data to the ****(User-Product level)****. This doesn't reduce data quality; rather, it focuses it into ****(Behavioral Features)**** that describe the user's relationship with the product over time—the optimal level for predictive repurchase decisions"
      ],
      "metadata": {
        "id": "ayxzQDrjEkcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Obtain a list of unique users\n",
        "all_users = My_Data_Aggregated['user_id'].unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RpYz-9mLEkcv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly mix users once (to ensure fair distribution) with the seed installed\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(all_users)"
      ],
      "metadata": {
        "trusted": true,
        "id": "8But5LapEkcv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying split points\n",
        "train_end = int(0.8 * len(all_users))\n",
        "val_end = int(0.9 * len(all_users))"
      ],
      "metadata": {
        "trusted": true,
        "id": "BqQzIJ0vEkcv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmenting the user list\n",
        "train_users = all_users[:train_end]\n",
        "val_users = all_users[train_end:val_end]\n",
        "test_users = all_users[val_end:]"
      ],
      "metadata": {
        "trusted": true,
        "id": "HWNPPJOpEkcv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting actual data based on users\n",
        "train_df = My_Data_Aggregated[My_Data_Aggregated['user_id'].isin(train_users)]\n",
        "val_df = My_Data_Aggregated[My_Data_Aggregated['user_id'].isin(val_users)]\n",
        "test_df = My_Data_Aggregated[My_Data_Aggregated['user_id'].isin(test_users)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "IeMXycvPEkcv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(My_Data_Aggregated))\n",
        "print(\"-\"*20)\n",
        "print(f\"{len(train_df)} {len(train_users)}\")\n",
        "print(f\"{len(val_df)} {len(val_users)}\")\n",
        "print(f\"{len(test_df)} {len(test_users)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "dU9SqK-hEkcw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adhering to the principle of (Time-aware Splitting) and to avoid (Data leakage), we split the data based on (User IDs) rather than randomly splitting rows. This ensures that the model is trained on a stable set of users and tested on a completely independent set, mimicking real-world scenarios where the system needs to predict (future orders) based on aggregated  (past history) , without any time overlap between the training and test sets."
      ],
      "metadata": {
        "id": "zkvO6L3qEkcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining the link between features and the goal\n",
        "correlation = train_df.corr()['target'].sort_values(ascending=False)\n",
        "print(correlation.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "kvfFXvpGEkcw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Completed\n",
        "We have successfully completed the full preprocessing study as outlined in the project requirements."
      ],
      "metadata": {
        "id": "DeErfRhM-dVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Task: Target Selection\n",
        "\n",
        "Target Variable: 'days_since_prior_order'\n",
        "\n",
        "\"We chose to predict the time interval between orders rather than reorder counts for its high strategic value. Predicting when a customer will return allows for:\n",
        "\n",
        "Precision Retargeting: Sending marketing incentives at the exact moment a customer is likely to restock.\n",
        "\n",
        "Dynamic Scheduling: Optimizing logistics and supply chain operations based on predicted temporal demand.\n",
        "\n",
        "Customer Churn Prevention: Identifying deviations from predicted ordering cycles to intervene before a customer stops using the platform.\""
      ],
      "metadata": {
        "id": "49oKMThD-dVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Isolation:\n",
        "I isolated behavioral features from identifiers to specifically target the days_since_prior_order prediction."
      ],
      "metadata": {
        "id": "G4kGB_6J-dVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection: Removing identifiers and the classification target\n",
        "# to isolate features for the regression task (predicting days).\n",
        "features = [col for col in My_Data_Aggregated.columns if col not in ['user_id', 'product_id', 'target', 'days_since_prior_order']]\n",
        "\n",
        "X = My_Data_Aggregated[features].fillna(0)\n",
        "y = My_Data_Aggregated['days_since_prior_order'].fillna(0)\n",
        "\n",
        "# Splitting the aggregated data into training and testing sets (80/20)\n",
        "# This ensures we evaluate the model on unseen user-product pairs.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Evaluation Function: Comparing models using MAE and R-squared metrics\n",
        "def evaluate_regression(name, y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"--- {name} Performance ---\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"R2 Score: {r2:.2f}\\n\")\n",
        "\n",
        "# RAM Optimization\n",
        "del X, My_Data_Aggregated\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "EiQ6HLea-dVS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methodology:\n",
        "OLS: Established the baseline performance.\n",
        "\n",
        "Lasso (L1): Integrated for automatic feature selection and noise reduction.\n",
        "\n",
        "Ridge (L2): Applied to stabilize the model against feature multi-collinearity and Elastic Net."
      ],
      "metadata": {
        "id": "oFFtUDTF-dVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 1: Ordinary Least Squares (Linear Regression) ---\n",
        "# Baseline model to understand the linear relationship between features and order timing.\n",
        "ols_reg = LinearRegression()\n",
        "ols_reg.fit(X_train, y_train)\n",
        "ols_pred = ols_reg.predict(X_test)\n",
        "\n",
        "# Numerical Evaluation\n",
        "evaluate_regression(\"Linear Regression (OLS)\", y_test, ols_pred)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, ols_pred, alpha=0.3, color='blue')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('OLS: Actual vs Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# RAM Optimization\n",
        "plt.close()\n",
        "del ols_reg\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ppGu4qme-dVS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 2: Lasso Regression (L1 Regularization) ---\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_train, y_train)\n",
        "lasso_pred = lasso_reg.predict(X_test)\n",
        "\n",
        "# Numerical Evaluation\n",
        "evaluate_regression(\"Lasso (L1)\", y_test, lasso_pred)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, lasso_pred, alpha=0.3, color='green')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Lasso: Actual vs Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# RAM Optimization\n",
        "plt.close()\n",
        "del lasso_reg\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "HSYlfieA-dVS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 3: Ridge Regression (L2 Regularization) ---\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "ridge_pred = ridge_reg.predict(X_test)\n",
        "\n",
        "# 1. Numerical Evaluation for comparison\n",
        "evaluate_regression(\"Ridge Regression (L2)\", y_test, ridge_pred)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, ridge_pred, alpha=0.3, color='orange')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Ridge: Actual vs Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# RAM Optimization\n",
        "plt.close()\n",
        "del ridge_reg\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "mHRQ6jCR-dVS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 4: Elastic Net Regression (L1 + L2) ---\n",
        "# Combined penalty useful for handling correlated features and feature selection simultaneously.\n",
        "elastic_reg = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elastic_reg.fit(X_train_scaled, y_train)\n",
        "elastic_pred = elastic_reg.predict(X_test_scaled)\n",
        "\n",
        "# Numerical Evaluation\n",
        "evaluate_regression(\"Elastic Net\", y_test, elastic_pred)\n",
        "\n",
        "# 2. Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, elastic_pred, alpha=0.3, color='cyan')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Elastic Net: Actual vs Predicted')\n",
        "plt.show()\n",
        "\n",
        "# RAM Optimization\n",
        "plt.close()\n",
        "del elastic_reg\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "1vIjd-ex-dVT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Regressor (SVR)"
      ],
      "metadata": {
        "id": "9czxdpYC-dVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StandardScaler:\n",
        "Mandatory for SVR to ensure all features contribute equally to distance calculations; otherwise, features with larger scales will bias the model, and Especially after the establishment Aggregation."
      ],
      "metadata": {
        "id": "LCLxLFu_-dVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling: SVR is extremely sensitive to the scale of data.\n",
        "# We must scale features to have a mean of 0 and variance of 1.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2lNz9Use-dVT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methodology:\n",
        "\n",
        "LinearSVR vs SVR (linear): LinearSVR is specifically optimized for large datasets, offering much faster computation and better memory efficiency than a standard SVR with a linear kernel.\n",
        "\n",
        "RBF Kernel: Used to capture complex, non-linear patterns by mapping data into higher-dimensional space, allowing the model to find relationships that linear models cannot detect."
      ],
      "metadata": {
        "id": "Mp_BuXQ--dVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 5: Linear SVR ---\n",
        "# Faster and optimized for linear relationships.\n",
        "l_svr = LinearSVR(dual=True, max_iter=1000, random_state=42)\n",
        "l_svr.fit(X_train_scaled, y_train)\n",
        "l_svr_pred = l_svr.predict(X_test_scaled)\n",
        "\n",
        "print(f\"Linear SVR MAE: {mean_absolute_error(y_test, l_svr_pred):.2f}\")\n",
        "\n",
        "evaluate_regression(\"Linear SVR\", y_test, l_svr_pred)\n",
        "\n",
        "# Required Visualization for SVR\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, l_svr_pred, alpha=0.3, color='blue')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Linear SVR: Actual vs Predicted')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Cleanup RAM\n",
        "del l_svr\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "gjfehiEu-dVT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL 6: SVR with RBF Kernel (Non-Linear) ---\n",
        "# CRITICAL: We use a subsample (X_train_svr) to prevent RAM explosion.\n",
        "rbf_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "rbf_svr.fit(X_train_svr, y_train_svr) # Using the 10,000 sample\n",
        "rbf_svr_pred = rbf_svr.predict(X_test_scaled)\n",
        "\n",
        "evaluate_regression(\"RBF SVR\", y_test, rbf_svr_pred)\n",
        "\n",
        "# Required Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, rbf_svr_pred, alpha=0.3, color='purple')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('RBF SVR: Actual vs Predicted')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Final Cleanup\n",
        "del rbf_svr\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IpRVn7kL-dVU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Nearest Neighbors (KNN)\n",
        "Distance-Based Logic: I applied StandardScaler before training because KNN is highly sensitive to feature scales when calculating distances.\n",
        "\n",
        "Metric Selection: I utilized Euclidean distance to find the most similar customer-product patterns.\n",
        "\n",
        "Computational Efficiency: Used n_jobs=-1 for parallel processing and performed"
      ],
      "metadata": {
        "id": "-wtIi0hc-dVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling: Mandatory for KNN as it is a distance-based algorithm\n",
        "# I am using the scaled data to ensure 'Euclidean' distance is calculated correctly\n",
        "\n",
        "k_values = [3, 7, 11]\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "for k in k_values:\n",
        "    for m in metrics:\n",
        "        # Initializing KNN with n_jobs=-1 to utilize all CPU cores for faster distance computation.\n",
        "        # We use the subsampled training set (X_train_svr) to prevent RAM exhaustion and speed up prediction.\n",
        "        knn = KNeighborsRegressor(n_neighbors=k, metric=m, n_jobs=-1)\n",
        "\n",
        "        # Fitting on the representative sample to ensure the model stays within memory limits\n",
        "        knn.fit(X_train_svr, y_train_svr)\n",
        "        knn_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "        # Evaluating the performance using MAE, RMSE, and R2 metrics\n",
        "        evaluate_regression(f\"KNN (k={k}, metric={m})\", y_test, knn_pred)\n",
        "\n",
        "        # Visualizing Actual vs Predicted values for each configuration\n",
        "        plt.figure(figsize=(5, 3))\n",
        "        plt.scatter(y_test, knn_pred, alpha=0.2, color='orange')\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n",
        "        plt.title(f\"KNN: k={k}, Metric={m}\")\n",
        "        plt.show()\n",
        "\n",
        "        # Memory Management\n",
        "        plt.close()\n",
        "        del knn_pred, knn\n",
        "        gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "w37tIiyy-dVU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Regressor\n"
      ],
      "metadata": {
        "id": "uEAMnUka-dVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL: Decision Tree Regressor ---\n",
        "# Decision Trees are non-parametric and do not require feature scaling.\n",
        "# We will test different depths to find the balance between underfitting and overfitting.\n",
        "\n",
        "depths = [3, 5, 10, None] # None means the tree grows until all leaves are pure\n",
        "\n",
        "for d in depths:\n",
        "\n",
        "    dt_reg = DecisionTreeRegressor(max_depth=d, random_state=42)\n",
        "\n",
        "    # Trees can handle larger datasets better than SVR/KNN,\n",
        "    # but we use X_train_scaled to stay consistent with previous models\n",
        "    dt_reg.fit(X_train_scaled, y_train)\n",
        "    dt_pred = dt_reg.predict(X_test_scaled)\n",
        "\n",
        "    # Numerical Evaluation\n",
        "    depth_label = d if d is not None else \"Unlimited\"\n",
        "    evaluate_regression(f\"Decision Tree (max_depth={depth_label})\", y_test, dt_pred)\n",
        "\n",
        "    # REQUIRED VISUALIZATION: Actual vs Predicted\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.scatter(y_test, dt_pred, alpha=0.2, color='teal')\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=1)\n",
        "    plt.title(f\"Decision Tree: max_depth={depth_label}\")\n",
        "    plt.xlabel(\"Actual\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.show()\n",
        "\n",
        "    # Memory Management: Critical for tree-based models\n",
        "    plt.close()\n",
        "    del dt_reg, dt_pred\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vzsCMtKw-dVU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Regressor\n"
      ],
      "metadata": {
        "id": "QRc3tUx8-dVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL: Random Forest Regressor ---\n",
        "# Note: This is computationally expensive; we limit n_estimators and use n_jobs=-1.\n",
        "\n",
        "# Defining the model\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Training on the scaled data\n",
        "rf_reg.fit(X_train_scaled, y_train)\n",
        "rf_pred = rf_reg.predict(X_test_scaled)\n",
        "\n",
        "# Numerical Evaluation\n",
        "evaluate_regression(\"Random Forest Regressor\", y_test, rf_pred)\n",
        "\n",
        "# Visualization: Actual vs Predicted\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, rf_pred, alpha=0.3, color='forestgreen')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Random Forest: Actual vs Predicted')\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "# CRITICAL RAM Optimization\n",
        "plt.close()\n",
        "del rf_reg\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xiGNoF2N-dVU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM Regressor\n",
        "Efficiency: \"LightGBM is chosen over XGBoost due to its superior speed and lower memory consumption, which is critical for the high-dimensional Instacart dataset.\"\n",
        "\n",
        "Hyperparameters: \"We use a balanced learning_rate and num_leaves to ensure the model captures complex patterns without overfitting to noise.\"\n",
        "\n",
        "Scalability: \"The use of n_jobs=-1 ensures the boosting process is parallelized across all CPU cores.\""
      ],
      "metadata": {
        "id": "qUyOq6Kd-dVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL: LightGBM Regressor ---\n",
        "# We use 'early_stopping' logic (via iterations) to prevent overfitting.\n",
        "\n",
        "# Defining the model\n",
        "lgb_reg = lgb.LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    importance_type='gain'\n",
        ")\n",
        "\n",
        "# Training\n",
        "lgb_reg.fit(X_train_scaled, y_train)\n",
        "lgb_pred = lgb_reg.predict(X_test_scaled)\n",
        "\n",
        "# Numerical Evaluation\n",
        "evaluate_regression(\"LightGBM (Gradient Boosting)\", y_test, lgb_pred)\n",
        "\n",
        "# Visualization: Actual vs Predicted\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(y_test, lgb_pred, alpha=0.3, color='darkred')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "plt.title('LightGBM: Actual vs Predicted')\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "# RAM Optimization\n",
        "plt.close()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CvUeNnhe-dVV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Strategy:\n",
        "Used MAE as the primary metric to provide a clear, business-ready interpretation of error in terms of \"days.\""
      ],
      "metadata": {
        "id": "QK6__vCQ-dVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL PERFORMANCE COMPARISON ---\n",
        "\n",
        "# Linear Variants\n",
        "evaluate_regression(\"Linear Regression (OLS)\", y_test, ols_pred)\n",
        "evaluate_regression(\"Lasso (L1)\", y_test, lasso_pred)\n",
        "evaluate_regression(\"Ridge (L2)\", y_test, ridge_pred)\n",
        "evaluate_regression(\"Elastic Net\", y_test, elastic_pred)\n",
        "\n",
        "# Support Vector Machines (SVR)\n",
        "evaluate_regression(\"Linear SVR\", y_test, l_svr_pred)\n",
        "evaluate_regression(\"RBF SVR\", y_test, rbf_svr_pred)\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "evaluate_regression(\"KNN Regressor\", y_test, knn_pred)\n",
        "\n",
        "# Tree-based Models\n",
        "evaluate_regression(\"Decision Tree\", y_test, dt_pred)\n",
        "evaluate_regression(\"Random Forest\", y_test, rf_pred)\n",
        "evaluate_regression(\"LightGBM (Boosting)\", y_test, lgb_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "x2JaQYFj-dVV"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}